tweets <- searchTwitter("#brexit", n=100, lang="en")
# Quedarse solo con el primer tweet para datos concretos del mismo
tweet <- tweets[[1]];
# Mostrar la estructura del tweet
#str(tweet)
# Obtener el texto del tweet:
tweet$getText()
# Obtener información acerca del usuario:
usuario <- getUser(tweet$getScreenName());
# Mostrar la estructura del usuario
#str(usuario)
# Obtener el nombre del usuario
usuario$getName()
tweets <- searchTwitter("#brexit", n=100, lang="en")
texto_tweets = sapply(tweets, function(x) x$getText())
head(texto_tweets)
cat("\nEliminamos retweet\n")
texto_tweets = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', texto_tweets)
head(texto_tweets)
cat("\nEliminar usuarios\n")
texto_tweets = gsub('@\\w+', '', texto_tweets)
head(texto_tweets)
cat("\nEliminamos puntuación\n")
texto_tweets = gsub('[[:punct:]]', '', texto_tweets)
head(texto_tweets)
cat("\nEliminamos números\n")
texto_tweets = gsub('[[:digit:]]', '', texto_tweets)
head(texto_tweets)
cat("\nEliminamos enlaces html\n")
texto_tweets = gsub('http\\w+', '', texto_tweets)
head(texto_tweets)
cat("\nEliminamos espacios innecesarios\n")
texto_tweets = gsub('[ \t]{2,}', '', texto_tweets)
texto_tweets = gsub('^\\s+|\\s+$', '', texto_tweets)
head(texto_tweets)
#Función para eliminar posibles errores al pasar a minúscula
try.error = function(x){
# creamos un missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not un error
if (!inherits(try_error, 'error'))
y = tolower(x)
return(y)
}
cat("\nPasamos a minúsucla si no hay error\n")
texto_tweets = sapply(texto_tweets, try.error)
head(texto_tweets)
cat("\nEliminamos NAs en el texto\n")
texto_tweets = texto_tweets[!is.na(texto_tweets)]
names(texto_tweets) = NULL
head(texto_tweets)
clasificacion_emociones = classify_emotion(texto_tweets, algorithm='bayes', prior=1.0)
emociones = clasificacion_emociones[,7]
# sustituimos NA's por 'unknown'
emociones[is.na(emociones)] = 'unknown'
clasificacion_popularidad = classify_polarity(texto_tweets, algorithm='bayes')
popularidad = clasificacion_popularidad[,4]
data = data.frame(text=texto_tweets, emotion=emociones, polarity=popularidad, stringsAsFactors=FALSE)
data = within(data, emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
ggplot(data, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette='Dark2') +
labs(x='Emotion categories', y='Number of tweets') +
ggtitle('Sentiment Analysis of Tweets about Brexit\n(classification by emotion)') +
theme(plot.title = element_text(size=12, face='bold'))
ggplot(data, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette='RdGy') +
labs(x='Polarity categories', y='Number of tweets') +
ggtitle('Sentiment Analysis of Tweets about Brexit\n(classification by polarity)') +
theme(plot.title = element_text(size=12, face='bold'))
stopwords
stopwords('english')
knitr::opts_chunk$set(echo = TRUE)
# Cargar la librería específica de TwitterR
library(twitteR);
# Leer el fichero de credenciales creado anteriormente, ¡cuidado con la ruta del fichero!.
source('../credenciales.R')
# Función que permite buscar: #hastag, @usuarios, palabras
tweets <- searchTwitter("#brexit", n=100, lang="en")
# Quedarse solo con el primer tweet para datos concretos del mismo
tweet <- tweets[[1]];
# Mostrar la estructura del tweet
#str(tweet)
# Obtener el texto del tweet:
tweet$getText()
# Obtener información acerca del usuario:
usuario <- getUser(tweet$getScreenName());
# Mostrar la estructura del usuario
#str(usuario)
# Obtener el nombre del usuario
usuario$getName()
tweets <- searchTwitter("#brexit", n=100, lang="en")
texto_tweets = sapply(tweets, function(x) x$getText())
head(texto_tweets)
cat("\nEliminamos retweet\n")
texto_tweets = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', texto_tweets)
head(texto_tweets)
cat("\nEliminar usuarios\n")
texto_tweets = gsub('@\\w+', '', texto_tweets)
head(texto_tweets)
cat("\nEliminamos puntuación\n")
texto_tweets = gsub('[[:punct:]]', '', texto_tweets)
head(texto_tweets)
cat("\nEliminamos números\n")
texto_tweets = gsub('[[:digit:]]', '', texto_tweets)
head(texto_tweets)
cat("\nEliminamos enlaces html\n")
texto_tweets = gsub('http\\w+', '', texto_tweets)
head(texto_tweets)
cat("\nEliminamos espacios innecesarios\n")
texto_tweets = gsub('[ \t]{2,}', '', texto_tweets)
texto_tweets = gsub('^\\s+|\\s+$', '', texto_tweets)
head(texto_tweets)
#Función para eliminar posibles errores al pasar a minúscula
try.error = function(x){
# creamos un missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not un error
if (!inherits(try_error, 'error'))
y = tolower(x)
return(y)
}
cat("\nPasamos a minúsucla si no hay error\n")
texto_tweets = sapply(texto_tweets, try.error)
head(texto_tweets)
cat("\nEliminamos NAs en el texto\n")
texto_tweets = texto_tweets[!is.na(texto_tweets)]
names(texto_tweets) = NULL
head(texto_tweets)
clasificacion_emociones = classify_emotion(texto_tweets, algorithm='bayes', prior=1.0)
emociones = clasificacion_emociones[,7]
# sustituimos NA's por 'unknown'
emociones[is.na(emociones)] = 'unknown'
clasificacion_popularidad = classify_polarity(texto_tweets, algorithm='bayes')
popularidad = clasificacion_popularidad[,4]
data = data.frame(text=texto_tweets, emotion=emociones, polarity=popularidad, stringsAsFactors=FALSE)
data = within(data, emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
ggplot(data, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette='Dark2') +
labs(x='Emotion categories', y='Number of tweets') +
ggtitle('Sentiment Analysis of Tweets about Brexit\n(classification by emotion)') +
theme(plot.title = element_text(size=12, face='bold'))
ggplot(data, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette='RdGy') +
labs(x='Polarity categories', y='Number of tweets') +
ggtitle('Sentiment Analysis of Tweets about Brexit\n(classification by polarity)') +
theme(plot.title = element_text(size=12, face='bold'))
emociones_data = levels(factor(data$emotion))
tama_emociones_data = length(emociones_data)
documento_emociones = rep('', tama_emociones_data)
for (i in 1:tama_emociones_data)
{
tmp = texto_tweets[emociones == emociones_data[i]]
documento_emociones[i] = paste(tmp, collapse=' ')
}
# eliminamos palabras vacias como or,as,off...
documento_emociones = removeWords(documento_emociones, stopwords('english'))
# Creamos el corpus
corpus = Corpus(VectorSource(documento_emociones))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = emociones_data
# comparison word cloud
comparison.cloud(tdm, colors = brewer.pal(tama_emociones_data, 'Dark2'),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
if (!require('pacman')) install.packages('pacman&')
pacman::p_load(devtools, installr)
if (!require('pacman')) install.packages('pacman')
pacman::p_load(twitteR, sentiment, plyr, ggplot2, wordcloud, RColorBrewer, httpuv, RCurl, base64enc)
options(RCurlOptions = list(cainfo = system.file('CurlSSL', 'cacert.pem', package = 'RCurl')))
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
knitr::opts_chunk$set(echo = TRUE)
if (!require('pacman')) install.packages('pacman&')
require('pacman)
#if (!require('pacman')) install.packages('pacman&')
require('pacman')
require('pacman')
if (!require('pacman')) install.packages('pacman')
pacman::p_load(twitteR, sentiment, plyr, ggplot2, wordcloud, RColorBrewer, httpuv, RCurl, base64enc)
options(RCurlOptions = list(cainfo = system.file('CurlSSL', 'cacert.pem', package = 'RCurl')))
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
shiny::runApp('Twitter')
runApp('Twitter')
runApp('Twitter')
shiny::runApp()
shiny::runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
xxx <- c(1,2,3)
yyyy <- c(2,3,2)
plot(xxx,yyyy)
runApp()
runApp()
runApp()
TweetFrame<-function(twtList){
df<- do.call("rbind",lapply(twtList,as.data.frame))
#removes emoticons
df$text <- sapply(df$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
df$text = gsub("(f|ht)tp(s?)://(.*)[.][a-z]+", "", df$text)
return (df$text)
}
pos.words=scan('positive-words.txt', what='character',comment.char=';')
neg.words=scan('negative-words.txt', what='character',comment.char=';')
wordDatabase<-function()
{
pos.words<<-c(pos.words, 'Congrats', 'prizes', 'prize', 'thanks', 'thnx', 'Grt', 'gr8', 'plz', 'trending', 'recovering', 'brainstorm', 'leader')
neg.words<<-c(neg.words, 'Fight', 'fighting', 'wtf', 'arrest', 'no', 'not')
}
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
list=lapply(sentences, function(sentence, pos.words, neg.words)
{
sentence = gsub('[[:punct:]]',' ',sentence)
sentence = gsub('[[:cntrl:]]','',sentence)
sentence = gsub('\\d+','',sentence)
sentence = gsub('\n','',sentence)
sentence = tolower(sentence)
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
pp=sum(pos.matches)
nn = sum(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
list1=c(score, pp, nn)
return (list1)
}, pos.words, neg.words)
score_new=lapply(list, `[[`, 1)
pp1=score=lapply(list, `[[`, 2)
nn1=score=lapply(list, `[[`, 3)
scores.df = data.frame(score=score_new, text=sentences)
positive.df = data.frame(Positive=pp1, text=sentences)
negative.df = data.frame(Negative=nn1, text=sentences)
list_df=list(scores.df, positive.df, negative.df)
return(list_df)
}
library(reshape)
sentimentAnalyser<-function(result)
{
#Creating a copy of result data frame
test1=result[[1]]
test2=result[[2]]
test3=result[[3]]
#Creating three different data frames for Score, Positive and Negative
#Removing text column from data frame
test1$text=NULL
test2$text=NULL
test3$text=NULL
#Storing the first row(Containing the sentiment scores) in variable q
q1=test1[1,]
q2=test2[1,]
q3=test3[1,]
qq1=melt(q1, ,var='Score')
qq2=melt(q2, ,var='Positive')
qq3=melt(q3, ,var='Negative')
qq1['Score'] = NULL
qq2['Positive'] = NULL
qq3['Negative'] = NULL
#Creating data frame
table1 = data.frame(Text=result[[1]]$text, Score=qq1)
table2 = data.frame(Text=result[[2]]$text, Score=qq2)
table3 = data.frame(Text=result[[3]]$text, Score=qq3)
#Merging three data frames into one
table_final=data.frame(Text=table1$Text, Positive=table2$value, Negative=table3$value, Score=table1$value)
return(table_final)
}
percentage<-function(table_final)
{
#Positive Percentage
#Renaming
posSc=table_final$Positive
negSc=table_final$Negative
#Adding column
table_final$PosPercent = posSc/ (posSc+negSc)
#Replacing Nan with zero
pp = table_final$PosPercent
pp[is.nan(pp)] <- 0
table_final$PosPercent = pp*100
#Negative Percentage
#Adding column
table_final$NegPercent = negSc/ (posSc+negSc)
#Replacing Nan with zero
nn = table_final$NegPercent
nn[is.nan(nn)] <- 0
table_final$NegPercent = nn*100
return(table_final)
}
wordDatabase()
twtList<-reactive({twtList<-searchTwitter(input$searchTerm, n=input$maxTweets, lang="en") })
tweets<-reactive({tweets<-TweetFrame(twtList() )})
tweets
TweetFrame(twtList() )
View(TweetFrame)
tweets
clean_text
result<-reactive({result<-score.sentiment(tweets(), pos.words, neg.words, .progress='none')})
table_final<-reactive({table_final<-sentimentAnalyser(  result() )})
table_final_percentage<-reactive({table_final_percentage<-percentage(  table_final() )})
output$tabledata<-renderTable(table_final_percentage())
toptrends<-function(place)
{
a_trends = availableTrendLocations()
woeid = a_trends[which(a_trends$name==place),3]
trend = getTrends(woeid)
trends = trend[1:2]
#To clean data and remove Non English words: (not required)
dat <- cbind(trends$name)
dat2 <- unlist(strsplit(dat, split=", "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
dat4 <- dat2[-dat3]
#dat5 <- trends[,which(trends$name==dat4)]
return(dat4)
}
trend_table<-reactive({trend_table<-toptrends(  input$trendingTable ) })
output$trendtable<-renderTable(trend_table() )
toptrends<-function(place)
{
a_trends = availableTrendLocations()
woeid = a_trends[which(a_trends$name==place),3]
trend = getTrends(woeid)
trends = trend[1:2]
#To clean data and remove Non English words: (not required)
dat <- cbind(trends$name)
dat2 <- unlist(strsplit(dat, split=", "))
dat3 <- grep("dat2", iconv(dat2, "latin1", "ASCII", sub="dat2"))
if(dat3==0)
return(dat2)
dat4 <- dat2[-dat3]
return(dat4)
}
trend_table<-reactive({trend_table<-toptrends(  input$trendingTable ) })
output$trendtable<-renderTable(trend_table() )
wordclouds<-function(text)
{
library(tm)
corpus<-Corpus(VectorSource(text))
#corpus
#inspect(corpus[1])
#clean text
clean_text <- tm_map(corpus, removePunctuation)
#clean_text <- tm_map(clean_text, content_transformation)
clean_text <- tm_map(clean_text, content_transformer(tolower))
clean_text <- tm_map(clean_text, removeWords, stopwords("english"))
clean_text <- tm_map(clean_text, removeNumbers)
clean_text <- tm_map(clean_text, stripWhitespace)
return(clean_text)
}
text_word<-reactive({text_word<-wordclouds(  tweets() ) })
text_word
wordclouds<-function(text)
{
library(tm)
corpus<-Corpus(VectorSource(text))
#corpus
#inspect(corpus[1])
#clean text
clean_text <- tm_map(corpus, removePunctuation)
#clean_text <- tm_map(clean_text, content_transformation)
clean_text <- tm_map(clean_text, content_transformer(tolower))
clean_text <- tm_map(clean_text, removeWords, stopwords("english"))
clean_text <- tm_map(clean_text, removeNumbers)
clean_text <- tm_map(clean_text, stripWhitespace)
head(clean_text)
return(clean_text)
}
text_word<-reactive({text_word<-wordclouds(  tweets() ) })
output$word<-renderPlot({wordcloud(text_word(), random.order=F,max.words=80, col=rainbow(100), scale=c(4.5,1.5))
})
runApp()
runApp()
runApp()
xxx <- c(1,2,3)
yyyy <- c(2,3,2)
hist(xxx)
runApp()
runApp()
runApp()
runApp()
analisis_sentimientos <- function(text){
corpus <- Corpus(VectorSource(text))
clasificacion_emociones = classify_emotion(corpus, algorithm='bayes', prior=1.0)
emociones = clasificacion_emociones[,7]
# sustituimos NA's por 'unknown'
emociones[is.na(emociones)] = 'unknown'
clasificacion_popularidad = classify_polarity(corpus, algorithm='bayes')
popularidad = clasificacion_popularidad[,4]
data = data.frame(text=corpus, emotion=emociones, polarity=popularidad, stringsAsFactors=FALSE)
data = within(data, emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
return(emociones)
}
analisis_sentimeintos_paso1 <- reactive({analisis_sentimeintos_paso1<-analisis_sentimientos( tweets() ) })
output$sentimientos <- renderPlot({ggplot(data, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette='Dark2') +
labs(x='Emotion categories', y='Number of tweets') +
ggtitle('Sentiment Analysis of Tweets about Brexit\n(classification by emotion)') +
theme(plot.title = element_text(size=12, face='bold'))})
renderPlot({ggplot(data, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette='Dark2') +
labs(x='Emotion categories', y='Number of tweets') +
ggtitle('Sentiment Analysis of Tweets about Brexit\n(classification by emotion)') +
theme(plot.title = element_text(size=12, face='bold'))})
runApp()
ggplot(analisis_sentimeintos_paso1, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette='Dark2') +
labs(x='Emotion categories', y='Number of tweets') +
ggtitle('Sentiment Analysis of Tweets about Brexit\n(classification by emotion)') +
theme(plot.title = element_text(size=12, face='bold'))
library(ggplot2)
ggplot(analisis_sentimeintos_paso1, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette='Dark2') +
labs(x='Emotion categories', y='Number of tweets') +
ggtitle('Sentiment Analysis of Tweets about Brexit\n(classification by emotion)') +
theme(plot.title = element_text(size=12, face='bold'))
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
help(ggsave)
runApp()
runApp()
runApp()
library(ggplot2)
library(dplyr)
library(purrr)
library(twitteR)
library(ggplot2)
library(dplyr)
library(purrr)
library(twitteR)
library(tidyr)
tweets_iph_an_ipa <- tweets_df %>%
select(id, statusSource, text, created) %>%
extract(statusSource, "source", "Twitter for (.*?)<") %>%
filter(source %in% c("iPhone", "Android", "iPad"))
library(tidyr)
library(stringr)
library(tidytext)
runApp()
runApp()
install.packages("rsconnect")
library(rsconnect)
library(rsconnect)
shiny::runApp()
rsconnect::setAccountInfo(name='masterdatcom2017',
token='19DC6E57271120AA7B6345A293C97AD3',
secret='JlyRO5yazn1t/ckLcoE0pNLY46yJpmz2qzfiUbI1')
setwd("~/Dropbox/zMaster/zRStudio/SistemasDeRecuperacionDeInformacionYRecomendacion/Twitter")
rsconnect::deployApp('~/Dropbox/zMaster/zRStudio/SistemasDeRecuperacionDeInformacionYRecomendacion/Twitter')
rsconnect::deployApp('~/Dropbox/zMaster/zRStudio/SistemasDeRecuperacionDeInformacionYRecomendacion/Twitter')
rsconnect::deployApp('~/Dropbox/zMaster/zRStudio/SistemasDeRecuperacionDeInformacionYRecomendacion/Twitter')
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
